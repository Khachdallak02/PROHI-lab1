{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 3 - Predicting survival chance for Titanic passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "student_name =   \"Khachatur Dallakyan\"\n",
    "student_id =     \"khda2699\"\n",
    "student_background =  \"technical\" #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revised the EDA (Homework 1) - one of the plots has been revised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) The data is sourced from Titanic Kaggle competition -> https://www.kaggle.com/competitions/titanic/data (for simplicity only train.csv was chosen - data has test/train splits for model training; train.csv is renamed to titanic.csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) The dataset originates from the famous sinking of Titanic in 1912. Dataset contains passenger information on Titanic passengers pressumably before it embarked from the port, primarily used for survival prediction analysis and demographic studies of the event.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) The dataset consists of two main files: train.csv and test.csv. train.csv was chosen as it additionally has the \"Survived\" column, as it also has more observations. The data is in CSV format and includes header with feature names. No additional metadata files are provided, but kaggle page has description for the features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) The training dataset contains 891 observations (passengers) and 12 features including the target feature 'Survived'. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Yes, the dataset contains numerical features:\n",
    "- Age: Passenger age in years\n",
    "- Fare: Ticket price\n",
    "- SibSp: Number of siblings/spouses aboard the Titanic\n",
    "- Parch: Number of parents/children aboard the Titanic\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Yes, the dataset contains both nominal and ordinal categorical features:\n",
    "Nominal:\n",
    "- Sex: male/female\n",
    "- Embarked: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)\n",
    "- Cabin: Cabin number\n",
    "Ordinal:\n",
    "- Pclass: Passenger class (1, 2, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7) The dataset contains one explicit binary feature:\n",
    "- Survived: 0 (=No/died) or 1 (=Yes/survived)\n",
    "Also additional binary features could be engineered from existing features; for example:\n",
    "- IsChild: Created using Age (1 if age < 18, 0 otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df_original = pd.read_csv('titanic.csv')\n",
    "df_original.info()\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_original.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PassengerId is just a unique indetifer; Name and Ticket column are inconsistent; difficult to process and hard to find pattern in. Cabin columns has around 700 missing values, which is majority of observation. Therefore droping those columns is good first step for analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.rename(columns={\n",
    "    'SibSp': 'Siblings_Spouses_Aboard',\n",
    "    'Parch': 'Parents_Children_Aboard'\n",
    "})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SibSp and Parch both have unintutive column names, renaming them makes easier to comprehend meaning of those features from the first glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['Age'])\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Age column has some observation with missing values. It was chosen to remove those observation with missing age column. After the filtration 714 observations remain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: What percentage of passengers were not adults?\n",
    "non_adult_passengers = df[df['Age'] < 18]\n",
    "percentage_non_adult = (len(non_adult_passengers) / len(df) * 100)\n",
    "\n",
    "print(f\"Percentage of not adults among passengers: {percentage_non_adult}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presumably not everyone was travelling with their families on titanic, but there were many travelling with families as well, so around 15% of young passenger is within range of expectations. This data gives us information about priority groups who should be saved first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: What is the average fare price paid by the passengers?\n",
    "#  What are other statistics related to fare price across all passengers?\n",
    "fare_stats = df['Fare'].describe()\n",
    "\n",
    "print(\"Fare Statistics:\")\n",
    "print(f\"Average fare price: £{fare_stats['mean']:.2f}\")\n",
    "print(f\"Median fare price: £{fare_stats['50%']:.2f}\")\n",
    "print(f\"Standard deviation: £{fare_stats['std']:.2f}\")\n",
    "print(f\"Minimum fare price: £{fare_stats['min']:.2f}\")\n",
    "print(f\"Maximum fare price: £{fare_stats['max']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nowadays boat tickets for long trips such as titanic might cost higher, but considering more than a century of inflation average ticket prices are reasonable. Standard deviation is also very high; but that also makes sense as ticket prices differed a lot for different classes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: How does the survival rate vary by passenger class?\n",
    "survival_by_class = df.groupby('Pclass').agg({\n",
    "    'Survived':['mean']})\n",
    "survival_by_class.columns = ['Survival Rate']\n",
    "survival_by_class['Survival Rate'] = (survival_by_class['Survival Rate'] * 100).round(2)\n",
    "\n",
    "print(survival_by_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that higher the class, higher the survival rate, as the 1st class has the highest survival rate followed by middle class and then the third class. It's intuitive, as higher classes had higer social status during victorian era and were given higher priority when evacuating. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: How does the survival rate vary by gender?\n",
    "gender_analysis = df.groupby('Sex').agg({'Survived': ['mean']})\n",
    "\n",
    "gender_analysis.columns = ['Survival Rate']\n",
    "gender_analysis['Survival Rate'] = (gender_analysis['Survival Rate'] * 100).round(2)\n",
    "\n",
    "print(gender_analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that females have way higher survivability rate. Again it's intuitive, as women were given priority during evacuation for lifeboats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 1: How does survival rate vary across passenger classes based on the embarked port?\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Pclass', y='Survived', hue='Embarked', data=df)\n",
    "plt.title('Survival Rate by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boxplot visualization has been chosen since y axis feature Survived is a binary feature. Visualization shows that those who embarked at C = Cherbourg had slightly higher chance of survival. However importantly boxplot visualization also shows the 95% confidence intervals, which in our case is sometimes very large, meaning conclusions about varying survivale rates cannot be certain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 2: How does survival rate vary by gender across different classes?\n",
    "plt.figure(figsize=(12, 7))\n",
    "sns.violinplot(x='Pclass', y='Survived', hue='Sex', data=df, split=True, inner='quart')\n",
    "plt.title('Survival Distribution by Class and Gender')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Survived (0=No, 1=Yes)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following recommendations given after homework 1 boxplot was replaced by violinplot. As we can see, female have higher survival rate then male passengers across all classes. However difference is more distinct for the 1st and 2nd class, compared to the 3rd class, where females also have higher casualty rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 3: How does fare distribution vary across passenger classes?\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='Pclass', y='Fare', data=df)\n",
    "plt.title('Fare Distribution by Passenger Class')\n",
    "plt.xlabel('Passenger Class')\n",
    "plt.ylabel('Fare')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For fair price distibution, boxplot is more approriate visualization tool, as Fare price is a numerical feature and we are interested in mean, standard deviation and outliers. Visualization shows that for the 2nd and 3rd class the price range is low and tickets are priced similarly, but for the 1st class average ticket price is considerably higher with some extremely high outliers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question 4: How does family size affect survival?\n",
    "df['Family_Size'] = df['Siblings_Spouses_Aboard'] + df['Parents_Children_Aboard'] + 1 # +1 to include passenger himself/herself\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='Family_Size', y='Survived', data=df)\n",
    "plt.title('Survival Rate by Family Size')\n",
    "plt.xlabel('Total Family Members Aboard')\n",
    "plt.ylabel('Survival Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again boxplot visualization has been chosen since y axis feature Survived is a binary feature. Family size is defined as Siblings_Spouses_Aboard + Parents_Children_Aboard + 1 to also include passenger. Visualization shows higher survival rates for those who had 1-3 family member aboard (2 <= Family_Size <= 4), compared to those who were alone aboard (Family_Size = 1). Families larger thay 5 had comparitively larger confidence interval, presumably because of low count of observations, so their average survival rate is less accurate prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson correlation between Age and Fare\n",
    "correlation = df['Age'].corr(df['Fare'])\n",
    "\n",
    "print(f\"Pearson correlation coefficient between Age and Fare: {correlation:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we approximate the coefficient toward ~0.1; we will have very small positive correlation, with small strength of association.\n",
    "This does make sense logically, because there are free tickets, which presumably are for children (so lower age correlate with lower prices). However correlation is very small becuase for adults price will rather correlate with class of passengers, rather than age. (For example 24 years old 1st class passenger would pay way more for the tickeet compared to 35 years old 3rd class passenger)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Binary target class will be \"Survived\" [0 (=No/died) or 1 (=Yes/survived)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['Pclass', 'Sex', 'Age',  'Fare']\n",
    "x = df[features]\n",
    "y = df['Survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will build a model that predicts Survival status of Titanic passenger from passenger class, sex, age and ticket price. The reason for selection of this specific input variables is because I beleive this can be the most important features to predict survival status. Individuals from higher class, females and children were given hgiher priority for evacuation, additionally fare ticket price might also contribute to the part of the ship passenger was one, therefore his/her survival rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "categorical_features = ['Pclass', 'Sex']\n",
    "numerical_features = ['Age', 'Fare']\n",
    "\n",
    "x_encoded = pd.get_dummies(x, columns=categorical_features)\n",
    "print(\"Data types after encoding:\")\n",
    "print(x_encoded.dtypes)\n",
    "\n",
    "print(\"First 5 rows of encoded dataframe:\")\n",
    "print(x_encoded.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(x=y)\n",
    "plt.title('Distribution of Survival')\n",
    "plt.xlabel('Survived (0=No, 1=Yes)')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks([0, 1], ['Did not survive', 'Survived'])\n",
    "\n",
    "unique, counts = np.unique(y, return_counts=True)\n",
    "class_distribution = dict(zip(unique, counts))\n",
    "for label, count in class_distribution.items():\n",
    "    plt.text(label, count, f\"{count} ({count/len(y)*100:.1f}%)\", \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class imbalance occurs when one class significantly outnumbers others in a dataset. Titanic dataset doesn't suffer from class imbalance. Despite distribution not being 50/50, it's still around 60/40, which is reasonable and no class i not significantly outnumbering the other. Usually imbalances can bias machine learning models toward predicting the majority class, potentially leading to high accuracy but poor performance on the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.asarray(x_encoded)\n",
    "y = np.asarray(y)\n",
    "\n",
    "print(\"x shape:\", x.shape)\n",
    "print(\"y shape:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train,  x_test, y_train, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stratified partitioning ensures the training and test sets maintain the same proportion of survivors/non-\n",
    "survivors as the original dataset and the proportion is not modified by sampling. This prevents sampling bias and ensures the model learns meaningful patterns that generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "MODELS_TO_TEST = {\n",
    "    \n",
    "    \"DT_3\": DecisionTreeClassifier(max_depth=3),\n",
    "    \"DT_5\": DecisionTreeClassifier(max_depth=5),\n",
    "\n",
    "    \"RF_10\": RandomForestClassifier(n_estimators=10, max_depth=5),\n",
    "    \"RF_100\": RandomForestClassifier(n_estimators=100, max_depth=5),\n",
    "    \n",
    "    \"SVM_lin\": Pipeline([\n",
    "                      ('scaler', MinMaxScaler()),\n",
    "                      ('estimator', SVC(kernel='linear'))\n",
    "                    ]),\n",
    "    \"SVM_rbf\": Pipeline([\n",
    "                      ('scaler', MinMaxScaler()),\n",
    "                      ('estimator', SVC(kernel='rbf'))\n",
    "                    ]),\n",
    "    \n",
    "    \"KNN_3\": Pipeline([\n",
    "                      ('scaler', MinMaxScaler()),\n",
    "                      ('estimator', KNeighborsClassifier(n_neighbors=3))\n",
    "                    ]),\n",
    "    \"KNN_7\": Pipeline([\n",
    "                      ('scaler', MinMaxScaler()),\n",
    "                      ('estimator', KNeighborsClassifier(n_neighbors=7))\n",
    "                    ]),\n",
    "    \n",
    "    \"GB_50\": GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, max_depth=3),\n",
    "    \"GB_100\": GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=5)\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "SCORING_METRICS = [\"accuracy\", \"precision_macro\", \"recall_macro\", \"f1_macro\"]\n",
    "NUMBER_OF_SPLITS = 10\n",
    "\n",
    "results_evaluation = pd.DataFrame({\n",
    "    \"classifier_name\": [],\n",
    "    \"fit_time\": [],\n",
    "    \"score_time\": [],\n",
    "    \"test_accuracy\": [],\n",
    "    \"test_precision_macro\": [],\n",
    "    \"test_recall_macro\": [],\n",
    "    \"test_f1_macro\": [],\n",
    "})\n",
    "for name, classifier in MODELS_TO_TEST.items():\n",
    "        print(f\"Currently training the classifier {name}.\")\n",
    "        \n",
    "        scores_cv = cross_validate(classifier, x_train, y_train, cv=NUMBER_OF_SPLITS, scoring=SCORING_METRICS)\n",
    "        \n",
    "        dict_this_result = {\n",
    "            \"classifier_name\": [name],\n",
    "        }\n",
    "        \n",
    "        for metric_name, score_per_fold in scores_cv.items():\n",
    "            dict_this_result[metric_name] = [score_per_fold.mean()]\n",
    "            \n",
    "        this_result = pd.DataFrame(dict_this_result)\n",
    "        \n",
    "        results_evaluation = pd.concat([results_evaluation, this_result], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Question 1: Which model provides the best balance between computational\n",
    "efficiency and predictive performance?*\n",
    "\n",
    "Analysis: Gradient Boosting with 50 estimators (GB_50) offers the best balance with the highest F1 score\n",
    "(0.815) while requiring moderate training time (0.097s). KNN_3 presents an excellent alternative with\n",
    "comparable performance (0.799 F1) but dramatically faster training (0.005s) - a 19x speed improvement with\n",
    "only 2% performance loss.\n",
    "\n",
    "*Question 2: How does the complexity of Random Forest (RF_100 vs RF_10) affect\n",
    "both prediction accuracy and evaluation time?*\n",
    "\n",
    "Analysis: Increasing RF estimators from 10 to 100 results in an 8.3x longer training time (0.025s -> 0.205s)\n",
    "for only a modest 1.7% F1 score improvement (0.792 -> 0.805). This diminishing return suggests RF_10 may\n",
    "be sufficient for many applications, especially when computational resources are limited.\n",
    "\n",
    "*Question 3: Which kernel choice (linear vs RBF) for SVM provides better\n",
    "performance on this classification task, and at what computational cost?*\n",
    "\n",
    "Analysis: Surprisingly, the linear SVM outperforms the RBF kernel on F1 score (0.766 vs 0.757) despite taking\n",
    "longer to train. The RBF kernel shows higher precision (0.842 vs 0.780) but poorer recall (0.749 vs 0.762),\n",
    "suggesting it's more conservative in its predictions for this particular classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting classifier with 50 estimators performs best with the given data by most of the testing metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = results_evaluation.loc[results_evaluation['classifier_name'] == 'GB_50']\n",
    "best_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boost model with 50 estimators performs best with given data with the highest test accuracy (82.7%) and F1 score (0.815). Even though it's the model with highest accuracy among the models tested, according to kaggle submissions there potentially can be models with up to 100% accuracy. Moreover model might have specifically higher accuracy with people from specific category (e.g. Female), while lower with the other categories, which is not tested by the current metrics used to choose the best model. At last, Gradient Boost by its nature is a \"black box\" (especially compared to Decision trees), which might be problematic for deploying in a scenario related to prediction of human survival chance, considering explainability is important in this case."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scvi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
